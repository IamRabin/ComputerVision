{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e7pkQAGC4S_",
        "colab_type": "text"
      },
      "source": [
        "#Building custom auto-encoder model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIDt8ykA-X0x",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T11eGmTkBjfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b3742621-1bee-4735-c3db-fc4c64d665a0"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from glob import glob\n",
        "%matplotlib inline\n",
        "from matplotlib.pyplot import imshow, show, subplots, close\n",
        "import seaborn as sns\n",
        "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gl07TkJoFKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The functions normalizes in the scale 0-1.\n",
        "\n",
        "def normalize(numpy_array):\n",
        "  minimum = numpy_array.min()\n",
        "  maximum = numpy_array.max()\n",
        "\n",
        "  normalized = (numpy_array - minimum)/(maximum - minimum)\n",
        "\n",
        "  return normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VNKjdn1S1iP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "train_images_dir = 'ComputerVision/auto-encoder/data/TRAIN'\n",
        "\n",
        "test_images_dir = 'ComputerVision/auto-encoder/data/TEST'\n",
        "\n",
        "neg_images_dir = 'ComputerVision/auto-encoder/data/NEG_TEST'\n",
        "\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 256\n",
        "INPUT_SHAPE = (127, 127)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D72c65IBhmUV",
        "colab_type": "text"
      },
      "source": [
        "#Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQMrZTwxdzuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_images = np.array([cv2.resize(cv2.imread(file, cv2.IMREAD_GRAYSCALE), INPUT_SHAPE) for file in glob(train_images_dir+'/*.jpg')])\n",
        "test_images = np.array([cv2.resize(cv2.imread(file, cv2.IMREAD_GRAYSCALE), INPUT_SHAPE) for file in glob(test_images_dir+'/*.jpg')])\n",
        "neg_images = np.array([cv2.resize(cv2.imread(file, cv2.IMREAD_GRAYSCALE), INPUT_SHAPE) for file in glob(neg_images_dir+'/*.jpg')])\n",
        "\n",
        "# Create a new channel axis, as the grayed don't have the channel axis and tf.keras image generator \n",
        "# requires matrix's shape with the channel.\n",
        "train_images = train_images[:,:,:,np.newaxis]\n",
        "test_images = test_images[:,:,:,np.newaxis]\n",
        "neg_images = neg_images[:,:,:,np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIbe70A-i1xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standarize Images with 0 mean and 1 variance\n",
        "mean = np.mean(train_images ,axis=(0, 1, 2, 3))\n",
        "std = np.std(train_images ,axis=(0, 1, 2, 3))\n",
        "train_images = (train_images - mean)/(std + 1e-7)\n",
        "test_images = (test_images - mean)/(std + 1e-7)\n",
        "neg_images = (neg_images - mean)/(std + 1e-7)\n",
        "\n",
        "# Normalize images\n",
        "train_images = normalize(train_images)\n",
        "test_images = normalize(test_images)\n",
        "neg_images = normalize(neg_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWWtnj7jY1K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training generator\n",
        "\n",
        "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=15, \n",
        "                                                            width_shift_range=0.10,\n",
        "                                                            height_shift_range=0.10,\n",
        "                                                            horizontal_flip=True,\n",
        "                                                            vertical_flip=True,\n",
        "                                                            zoom_range=0.10)\n",
        "\n",
        "train_generator = train_gen.flow(   \n",
        "    train_images,\n",
        "    train_images,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAy6Owdofeh3",
        "colab_type": "text"
      },
      "source": [
        "# Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyQUru5OCNUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model for encoder\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, fillters=[32, 64, 128, 196, 256, 1024], kernel_sizes=[3, 3, 3, 3, 3, 3], strides=[2, 2, 2, 2, 2, 2],\n",
        "               input_shape=INPUT_SHAPE, num_of_layers=6):\n",
        "    \n",
        "\n",
        "    super(Encoder, self).__init__()\n",
        "    # Since number of fillters is number of layers\n",
        "    # then check whether it is equal\n",
        "    # if it is different then raise the assertion exception\n",
        "    # These parts are not essential but useful for the sake of\n",
        "    # debuging\n",
        "    assert(len(fillters) == num_of_layers)\n",
        "    assert(len(kernel_sizes) == len(fillters))\n",
        "    assert(len(strides) == len(fillters))\n",
        "    # Just to not repeat \n",
        "    activation = 'selu'\n",
        "    # It can look pretty complicated, but it is just a repeated convolutional layer\n",
        "    self.conv_layer_1 = tf.keras.layers.Conv2D(fillters[0], (kernel_sizes[0], kernel_sizes[0]), \n",
        "                                               strides=(strides[0], strides[0]),\n",
        "                                               activation=activation)\n",
        "    self.conv_layer_2 = tf.keras.layers.Conv2D(fillters[1], (kernel_sizes[1], kernel_sizes[1]), \n",
        "                                               strides=(strides[1], strides[1]),\n",
        "                                               activation=activation)\n",
        "    self.conv_layer_3 = tf.keras.layers.Conv2D(fillters[2], (kernel_sizes[2], kernel_sizes[2]), \n",
        "                                               strides=(strides[2], strides[2]),\n",
        "                                               activation=activation)\n",
        "    self.conv_layer_4 = tf.keras.layers.Conv2D(fillters[3], (kernel_sizes[3], kernel_sizes[3]), \n",
        "                                               strides=(strides[3], strides[3]),\n",
        "                                               activation=activation)  \n",
        "    self.conv_layer_5 = tf.keras.layers.Conv2D(fillters[4], (kernel_sizes[4], kernel_sizes[4]), \n",
        "                                               strides=(strides[4], strides[4]),\n",
        "                                               activation=activation) \n",
        "    self.conv_layer_6 = tf.keras.layers.Conv2D(fillters[5], (kernel_sizes[5], kernel_sizes[5]), \n",
        "                                               strides=(strides[5], strides[5]),\n",
        "                                               activation=activation) \n",
        "    # same fpor batch normalization layer\n",
        "    self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_3 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_4 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_5 = tf.keras.layers.BatchNormalization()            \n",
        "    self.batch_norm_6 = tf.keras.layers.BatchNormalization()                                         \n",
        "\n",
        "\n",
        "    # Calculating the kernel size of the last layer and equating it to the shape of the layer's 2 output\n",
        "    # It is needed for reshaping the output of the decoder model\n",
        "    size = input_shape[0]\n",
        "    for kernel_size, stride in zip(kernel_sizes, strides):\n",
        "      size = tf.math.floor((size - kernel_size)/stride) + 1\n",
        "    self.last_layer_kernel = int(size)\n",
        "    self.flatten_layer = tf.keras.layers.Flatten()\n",
        "    self.dense_layer_1 = tf.keras.layers.Dense(self.last_layer_kernel**2 * fillters[-1]/8 * 8, activation=activation)\n",
        "    self.dense_layer_2 = tf.keras.layers.Dense(self.last_layer_kernel**2 * fillters[-1]/8 * 4, activation=activation)\n",
        "    # This is definition of the output layer, that defines the latent variable in our encoder model\n",
        "    self.dense_layer_out = tf.keras.layers.Dense(self.last_layer_kernel**2 * fillters[-1]/8 * 2)\n",
        "\n",
        "  def last_layer_kernel_shape(self):\n",
        "    \"\"\"\n",
        "    Returns the calculated kernel size\n",
        "    for the last layer, necessary for the\n",
        "    first layer in the decoder.\n",
        "    \"\"\"\n",
        "    return self.last_layer_kernel\n",
        "\n",
        "  # Remember to pass training=True in the training loop!\n",
        "  # otherwise the batch_norm won't work\n",
        "  def call(self, inputs, training=False):\n",
        "\n",
        "    # Pass the input throught the layers\n",
        "    x = self.conv_layer_1(inputs)\n",
        "    x = self.batch_norm_1(x, training=training)\n",
        "    x = self.conv_layer_2(x)\n",
        "    x = self.batch_norm_2(x, training=training)\n",
        "    x = self.conv_layer_3(x)\n",
        "    x = self.batch_norm_3(x, training=training)\n",
        "    x = self.conv_layer_4(x)\n",
        "    x = self.batch_norm_4(x, training=training)\n",
        "    x = self.conv_layer_5(x)\n",
        "    x = self.batch_norm_5(x, training=training)\n",
        "    x = self.conv_layer_6(x)\n",
        "    x = self.batch_norm_6(x, training=training)\n",
        "    x = self.flatten_layer(x)\n",
        "    x = self.dense_layer_1(x)\n",
        "    x = self.dense_layer_2(x)\n",
        "    # This is our output layer, which is the latent variable\n",
        "    x = self.dense_layer_out(x)    \n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxBYRvIfZWd",
        "colab_type": "text"
      },
      "source": [
        "# Decoder Part\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaUEK-HHReAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Creating an encoder model\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, fillters=[1024, 256, 196, 128, 64, 32, 1], kernel_sizes=[3, 3, 3, 3, 3, 3, 1], strides=[2, 2, 2, 2, 2, 2, 1],\n",
        "               input_kernel=None, num_of_layers=7):\n",
        "    \n",
        "   \n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    assert(len(fillters) == num_of_layers)\n",
        "    assert(len(kernel_sizes) == len(fillters))\n",
        "    assert(len(strides) == len(fillters))\n",
        "    activation = 'selu'\n",
        "    # Adjust the output_padding when you change the strides \n",
        "    # Or adjust the image site, which is usally easier\n",
        "    self.conv_layer_1 = tf.keras.layers.Conv2DTranspose(fillters[0], (kernel_sizes[0], kernel_sizes[0]),\n",
        "                                                        strides=(strides[0], strides[0]), \n",
        "                                                        activation=activation)\n",
        "    self.conv_layer_2 = tf.keras.layers.Conv2DTranspose(fillters[1], (kernel_sizes[1], kernel_sizes[1]),\n",
        "                                                        strides=(strides[1], strides[1]), \n",
        "                                                        activation=activation)\n",
        "    self.conv_layer_3 = tf.keras.layers.Conv2DTranspose(fillters[2], (kernel_sizes[2], kernel_sizes[2]),\n",
        "                                                        strides=(strides[2], strides[2]), \n",
        "                                                        activation=activation)\n",
        "    self.conv_layer_4 = tf.keras.layers.Conv2DTranspose(fillters[3], (kernel_sizes[3], kernel_sizes[3]),\n",
        "                                                        strides=(strides[3], strides[3]), \n",
        "                                                        activation=activation)\n",
        "    self.conv_layer_5 = tf.keras.layers.Conv2DTranspose(fillters[4], (kernel_sizes[4], kernel_sizes[4]),\n",
        "                                                        strides=(strides[4], strides[4]), \n",
        "                                                        activation=activation)\n",
        "    self.conv_layer_6 = tf.keras.layers.Conv2DTranspose(fillters[5], (kernel_sizes[5], kernel_sizes[5]),\n",
        "                                                        strides=(strides[5], strides[5]), \n",
        "                                                        activation=activation)\n",
        "    # There is an additional layer that wraps up the autoencoder's signal and creates the output image\n",
        "    self.conv_layer_7 = tf.keras.layers.Conv2DTranspose(fillters[6], (kernel_sizes[6], kernel_sizes[6]),\n",
        "                                                        strides=(strides[6], strides[6]), \n",
        "                                                        activation='sigmoid')\n",
        "    \n",
        "    # An activation for the input\n",
        "    self.activ = tf.keras.layers.Activation(activation)\n",
        "    self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_3 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_4 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_5 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_6 = tf.keras.layers.BatchNormalization()\n",
        "    self.batch_norm_7 = tf.keras.layers.BatchNormalization()\n",
        "    # The \"input_kernel\" parameter is from the previous layer\n",
        "    # The Dense's output is made so that, it fits the size of the reshaped image \n",
        "    # the reshaped image  - (input kernel, input kernel, fillters[0])\n",
        "    self.dense_layer_1 = tf.keras.layers.Dense(input_kernel**2 * fillters[0]/8 * 8, activation=activation)\n",
        "    self.reshape_layer = tf.keras.layers.Reshape((input_kernel, input_kernel, fillters[0]))\n",
        "    \n",
        "  # Remember to pass the training=True in the training loop!\n",
        "  def call(self, inputs, training=False):\n",
        "\n",
        "    # Pass the input throught the layers  \n",
        "    x = self.activ(inputs) \n",
        "    x = self.dense_layer_1(x)\n",
        "    x = self.reshape_layer(x)\n",
        "    x = self.batch_norm_1(x, training=training)\n",
        "    x = self.conv_layer_1(x)\n",
        "    x = self.batch_norm_2(x, training=training)\n",
        "    x = self.conv_layer_2(x)\n",
        "    x = self.batch_norm_3(x, training=training)\n",
        "    x = self.conv_layer_3(x)\n",
        "    x = self.batch_norm_4(x, training=training)\n",
        "    x = self.conv_layer_4(x)\n",
        "    x = self.batch_norm_5(x, training=training)\n",
        "    x = self.conv_layer_5(x)\n",
        "    x = self.batch_norm_6(x, training=training)\n",
        "    x = self.conv_layer_6(x)\n",
        "    x = self.batch_norm_7(x, training=training)\n",
        "    x = self.conv_layer_7(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCkfqpo6gvZo",
        "colab_type": "text"
      },
      "source": [
        "# Creating model objects\n",
        "Before we begin the training, we create the objects associated with the encoder and decoder. Those objects will be used for training, testing, and evaluation of the model. we just create an instance of the `Encoder` and `Decoder` classes and assign them to variables. If we want to pass some parameters, we just pass them to the created instance like in the case of `Decoder`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6amorb40L_Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder()\n",
        "# The last_layer_kernel_shape passes the shape of the last convolutional layer.\n",
        "# It is to ensure that the output will be the same for the first transpose convolutional\n",
        "# layer in the Decoder.\n",
        "last_layer_kernel_shape = encoder.last_layer_kernel_shape()\n",
        "decoder = Decoder(input_kernel=last_layer_kernel_shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvM223GFguWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer and loss function\n",
        "learning_rate = 0.00015\n",
        "ae_optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
        "# MSE works here best\n",
        "loss_func = tf.keras.losses.MeanSquaredError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeKyGAjlj7Jj",
        "colab_type": "text"
      },
      "source": [
        "# Custom training loop\n",
        "\n",
        "## One training step - so where the backpropagation is performed\n",
        "In the TF2 you're allowed to build loops where you can easily train any model. The loop consists of a part of code where backpropagation is performed and I call it a `train_step` and define it as a function. The rest of the training loop is described later.\n",
        "\n",
        "The backpropagation part or `train_step` consists of two parts, the part where the loss is evaluated and the part where the gradient is calculated and applied to the weights.\n",
        "The first part is always defined within the `with tf.GradientTape() as` statement. You can call the tape under any name, not only `ae_tape`, so the layer does not matter. Then under the statement, you define the operations where backpropagation is performed, so the loss functions and encoder and decoder. Then you assign the trained variables to a variable (as they are in a form of a list, you can join the list with the `+` sign). There is a small remark I would say, that the variables don't need to be always passed to the gradient evaluation, for example in some cases (generator in GAN networks) only part of the weights are updated, e.g. generator, where decoder would be left not updated. Then you would assign to the trainable variables only `trainable_vars = generator.trainable_variables`. \n",
        "\n",
        "The second part is calculating the gradient and applying it. With the tape variable ae_tape you call `gradient` and pass the loss and variables and it outputs gradients. Then you apply the gradients with optimizer of your choice as a gradient - variable tuple, using Python's zip method. Furthermore, you can also modify the gradient, e.g. apply a gradient clipping to the `ae_grads` variable or remove all the NaNs (which is not recommended) from the gradient. \n",
        "\n",
        "## Why training step as a function\n",
        "\n",
        "It is not only more convenient to make the training step a function, but also you can use the @tf.function decorator. It ensures that the graph is built in a non-eager model, which makes the computation far faster (sometimes tenfold). Nevertheless, it is almost impossible to debug it if the decorator is applied, thus while debugging, just comment out the decorator. \n",
        "Furthermore, you can pass any number of parameters you like, and process them in the function to get the desired result. Thus not only input but target variables are also allowed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiJLY5y6PPDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def train_step(input, target):\n",
        "  with tf.GradientTape() as ae_tape:\n",
        "    # Pass the input to the encoder\n",
        "    # Ensure for the training=True\n",
        "    # Otherwise the batchnorm won't work\n",
        "    encode_inp = encoder(input, training=True)\n",
        "    # Pass the encoder's output to the decoder\n",
        "    decode_inp = decoder(encode_inp, training=True)\n",
        "    # The prediction variable is just for clarity\n",
        "    prediction = decode_inp\n",
        "    # Calculate loss\n",
        "    loss = loss_func(input, prediction)\n",
        "    # Get the encoder and decoder variables\n",
        "    trainable_vars = encoder.trainable_variables \\\n",
        "                     + decoder.trainable_variables\n",
        "  # Calculate gradient          \n",
        "  ae_grads = ae_tape.gradient(loss, trainable_vars)\n",
        "  # And then apply the gradient to change the weights\n",
        "  ae_optimizer.apply_gradients(zip(ae_grads, trainable_vars))\n",
        "\n",
        "  # Loss is returned to monitor it while training\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFmCuYrkv2dH",
        "colab_type": "text"
      },
      "source": [
        "## Training loop \n",
        "After defining one training step, you define the whole training process.\n",
        "Usually, you create two loops, where one is responsible for dividing the whole process into epochs, and the other into batches. \n",
        "In the batch loop, you call the training step and pass the input and output (since here we have an autoencoder, the input is the same as output, so you could just pass twice the input). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4ZsSiWmOcXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of iters is here to iterate through all the \n",
        "# exaples in the dataset. If you won't call the generator\n",
        "# previously it can throw an exception.\n",
        "train_generator.next()\n",
        "num_of_iters = int((max(train_generator.index_array) + 1)/BATCH_SIZE)\n",
        "loss_array = []\n",
        "loss_array_disc = []\n",
        "print(\"Starting training ...\")\n",
        "for epoch in range(EPOCHS):\n",
        "  # Just total loss\n",
        "  loss_tot = 0\n",
        "  # Iterate through all the examples\n",
        "  for iteration, (input, target) in enumerate(train_generator):\n",
        "    # Break if the number of computed batches exceeds the\n",
        "    # total number of the examples\n",
        "    if iteration >= num_of_iters:\n",
        "      break\n",
        "    loss = train_step(input, target)\n",
        "    loss_tot += loss\n",
        "  loss_array.append((epoch, float(loss_tot)))\n",
        "  # Print the total loss every epoch\n",
        "  print('\\nLoss for epoch {0} is {1}'.format(epoch, loss_tot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reaKI8XbxC0E",
        "colab_type": "text"
      },
      "source": [
        "# After the training\n",
        "\n",
        "Here evaluation and testing of the model is performed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfCAIdvtXDx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ploting the loss versus epochs\n",
        "df = pd.DataFrame(np.array(loss_array), columns=['Epoch' ,'Loss'])\n",
        "sns.lineplot(data=df, x='Epoch', y='Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8_IJuKQ208k",
        "colab_type": "text"
      },
      "source": [
        "Here you can visualize the reconstructed image on the test images, it is usually helpful to examine them if they are reconstructed properly. Of course, they won't be in 100% the same, but the major features (like the shape of a bottle) should be reconstructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7G_fa_x9XSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take one test example\n",
        "y = test_images\n",
        "# Pass to the trained encoder\n",
        "e = encoder(y)\n",
        "# pass the encoded value back to the decoder\n",
        "d = decoder(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNqbZN019pMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show decoded image\n",
        "# Just change the index from 0 to n to see other images\n",
        "imshow(np.squeeze(d[0]), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VewgPg9pThM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show oryginal image\n",
        "imshow(np.squeeze(y[0]), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldn34fat3j-y",
        "colab_type": "text"
      },
      "source": [
        "# Showing the reduced representation of the latent space\n",
        "Using PCA you can visualize how the latent space looks like. Similar examples should cluster with other similar examples due to the clustering properties of the autoencoder. The clustering won't be the highest quality, yet there should be any.\n",
        "\n",
        "The plot represents negative (non-bottles) and positive (bottles) test examples as blue and orange. You can see that orange examples cluster together with the blue examples. Sometimes there are outliers and it is better to change the axis scale to see the clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX0b6p-vPK14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_trian = np.squeeze(encoder(train_images).numpy())\n",
        "encoded_test = np.squeeze(encoder(test_images))\n",
        "encoded_neg = np.squeeze(encoder(neg_images))\n",
        "new_encoded = np.stack([encoded_test, encoded_neg])\n",
        "pca = PCA(n_components=2)\n",
        "pca_results = pca.fit_transform(encoded_trian)\n",
        "pca_neg = pca.transform(encoded_neg)\n",
        "pca_pos = pca.transform(encoded_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmnBoRJ7Pqgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_pos = [('pos', x[0], x[1])for x in pca_pos]\n",
        "pca_neg = [('neg', x[0], x[1])for x in pca_neg]\n",
        "df_pca = pd.DataFrame(pca_neg+pca_pos, columns=['class', 'x', 'y'])\n",
        "sns.scatterplot(data=df_pca, x='x', y='y', hue='class')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A0p75v58Sl3",
        "colab_type": "text"
      },
      "source": [
        "# Creating a GMM model for the outlier detection \n",
        "To detect the outliers we will use the Gaussian Mixture Model, which allows for clustering multi-dimensional data and classifying them with a probabilistic model using Gaussian distributions (the number of them is specified with n_components). \n",
        "To read more see this page: https://scikit-learn.org/stable/modules/mixture.html\n",
        "\n",
        "The most important parameters here are the number of the components that, describe the clustered data and n_init which a number of initializations (usually the more, the better but requires more time). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJiyF9w-dAeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a GMM model for the N gaussian components\n",
        "# You can play with the number of components to get the right accuracy\n",
        "# You may lower the n_init number to lower the time needed for training\n",
        "# nevertheless, the overall performance can be lower.\n",
        "gmm = GaussianMixture(n_components=6, tol=10e-10, max_iter=600, n_init=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geqhdZuzNXBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pass the images from the dataset with the bottles\n",
        "# And get the encoder's latent variable\n",
        "encoded_trian = np.squeeze(encoder(train_images))\n",
        "encoded_trian_bott = np.squeeze(encoder(train_images))\n",
        "# Fit the gmm model with the latent variable generated using the training set\n",
        "gmm.fit(encoded_trian)\n",
        "# Calculate the \n",
        "Train_score = gmm.score_samples(encoded_trian_bott)\n",
        "# With the standard deviation you can control the sensitivity and specificity\n",
        "# for the GMM model.\n",
        "threshold = Train_score.mean() - 3 * Train_score.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD4zJw3LNwmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pos = np.squeeze(encoder(test_images))\n",
        "test_neg = np.squeeze(encoder(neg_images))\n",
        "score_pos = gmm.score_samples(test_pos)\n",
        "score_neg = gmm.score_samples(test_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQPGz-ItPwaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e8753d33-91f1-4bc1-91be-7b70567f61c2"
      },
      "source": [
        "pos_acc = np.sum(score_pos < threshold)/score_pos.shape[0]\n",
        "neg_acc = np.sum(score_neg > threshold)/score_neg.shape[0]\n",
        "all_acc = (np.sum(score_neg > threshold)+np.sum(score_pos < threshold))/(score_neg.shape[0]+score_pos.shape[0])\n",
        "print('Sensitivity (detecting bottles): {}'.format(pos_acc))\n",
        "print('Specificity (detecting non-bottle things): {}'.format(neg_acc))\n",
        "print('Accuracy: {}'.format(all_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitivity (detecting bottles): 0.7\n",
            "Specificity (detecting non-bottle things): 0.825\n",
            "Accuracy: 0.7625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IveCb0_2MRIB",
        "colab_type": "text"
      },
      "source": [
        "# Saving the model and using it outside the training protocol\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIN1LQRMQEgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the configuration\n",
        "import json\n",
        "## Encoder\n",
        "encoder_dict = {'fillters': [32, 64, 128, 196, 256, 1024],\n",
        "                'kernel_sizes': [3, 3, 3, 3, 3, 3],\n",
        "                'strides': [2, 2, 2, 2, 2, 2],\n",
        "                'num_of_layers': 6}\n",
        "with open('encoder_config.json', 'w') as fp:\n",
        "    json.dump(encoder_dict, fp)\n",
        "\n",
        "## Decoder\n",
        "decoder_dict = {'fillters': [1024, 256, 196, 128, 64, 32, 1], \n",
        "                'kernel_sizes': [3, 3, 3, 3, 3, 3, 1],\n",
        "                'strides': [2, 2, 2, 2, 2, 2, 1],\n",
        "                'num_of_layers': 7}\n",
        "with open('decoder_config.json', 'w') as fp:\n",
        "    json.dump(decoder_dict, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq3XUxynTaWP",
        "colab_type": "text"
      },
      "source": [
        "# Saving weights\n",
        "After the configuration is saved, you can save also the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSxIuZa-MWij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model\n",
        "## Encoder\n",
        "encoder.save_weights(filepath='encoder_model.tf')\n",
        "## Decoder\n",
        "decoder.save_weights(filepath='decoder_model.tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vAN8B77ThSj",
        "colab_type": "text"
      },
      "source": [
        "# Loading the model\n",
        "As previously mentioned, it is a bit complicated, although we are ready for it, so it should go smoothly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXYjVQriP4nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e551d4e7-7392-4228-f0af-dbe831b15188"
      },
      "source": [
        "# Loading the model\n",
        "## Load the config\n",
        "with open('encoder_config.json', 'r') as fp:\n",
        "    encoder_config = json.load(fp)\n",
        "with open('decoder_config.json', 'r') as fp:\n",
        "    decoder_config = json.load(fp)\n",
        "\n",
        "## Initialize encoder and decoder model objects\n",
        "## The ** means unpacking the dictionary\n",
        "encoder_new = Encoder(**encoder_config, input_shape=(127, 127))\n",
        "### Pass a zeros vector to initialize the weight's shapes\n",
        "latent = encoder(tf.zeros((1, 127, 127, 1)))\n",
        "last_layer_shape = encoder_new.last_layer_kernel_shape()\n",
        "decoder_new = Decoder(**decoder_config, input_kernel=last_layer_shape)\n",
        "### Pass a zeros vector to initialize the weight's shapes\n",
        "decoded = decoder_new(latent)\n",
        "## Load the weights\n",
        "encoder_new.load_weights(filepath='encoder_model.tf')\n",
        "decoder_new.load_weights(filepath='decoder_model.tf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f60c05c5860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12LfKrFtXOjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the loaded models\n",
        "latent = encoder_new(test_images[np.newaxis, 0, :, :])\n",
        "decoded = decoder_new(latent)\n",
        "imshow(np.squeeze(decoded[0]), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}